{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26c7758a",
   "metadata": {},
   "source": [
    "### 트랜스포머 인코더 모델 이해가기 목차\n",
    "\n",
    "* [Chapter 1 트랜스포머 개요)](#chapter1)\n",
    "* [Chapter 2 어텐션 메커니즘](#chapter2)\n",
    "   * [Section 2.1 잔차 블록 만들기](#section_2_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029e3e6d",
   "metadata": {},
   "source": [
    "### Chapter 1 트랜스포머 개요 <a class=\"anchor\" id=\"chapter1\"></a>\n",
    "1. 트랜스포머(transformer)는 문장이나 소리 등 순서가 있는 데이터에서 중요한 부분을 자동으로 찾아내 데이터를 처리하는 딥러닝모델이다.\n",
    "\n",
    "2. 기존 시간적 의존설을 다루기 위해 사용했던 순환신경망(RNN Recurrent Nerual Network)의 순차적인 처리방식과는 다르다.\n",
    "   - 모든 데이터를(병렬로) 처리해 RNN보다 훨씬 빠르고 효율적으로 학습할 수 있다.\n",
    "\n",
    "3. 데이터를 이해햐는 단계인 인코더와 인코더가 이해한 내용을 바탕으로 결과를 생성하는 디코더 구조로 이루어져 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96781031",
   "metadata": {},
   "source": [
    "### Chapter 2 어텐션 메커니즘 <a class=\"anchor\" id=\"chapter2\"></a>\n",
    "1. 트랜스포머의 핵심은 어텐션(attention)이라는 개념에 있다.\n",
    "\n",
    "2. 어텐션 메커니즘은 모델에 입력된 데이터의 모든 단어들 중 특정 단어와 관련이 높은 단어에 집중해 데이터를 처리하도록 설계된 기법이다.\n",
    "\n",
    "3. 셀프 어텐션\n",
    "   - 문장의 모든 단어가 서로를 참고해 각각 다른 단어와의 관련성을 파악하는 방법\n",
    "\n",
    "4. 멀티 어텐션\n",
    "   - 여러 개의 셀프 어텐션을 동시에 수행해 이 관련성을 다양한 관점에서 더욱 깊게 이해할 수 있도록 확장한 방법\n",
    "\n",
    "5. 인코더-디코더 순환 신경망(RNN)을 사용하던 기계 번역의 성능을 높이기 위해 고안되었다.\n",
    "   - 기계 번역: 사람이 사용하는 특정 언어를 컴퓨터가 다른 언어로 변환하는 것\n",
    "\n",
    "6. 기계 번역에서 인코더 RNN은 입력된 문장을 처리하여 최종 은닉상태(hidden state)로 만든다.\n",
    "   - 문맥 벡터(context vectpr)라고 부른다.\n",
    "   - 문맥 벡터는 문장의 중요 정보가 담긴 일종의 요약본이라고 할 수 있다.\n",
    "   - 순환 신경망(RNN)에서는 싱경망의 층(layer)를 종종 셀(cell)이라고 부른다.\n",
    "   \n",
    "     ![문맥 벡터](image/04-01-ContextVector.png)   \n",
    "\n",
    "7. 긴 문장일수록 번영기 어려울 수 있다.\n",
    "   - RNN 인코더에 입력되는 텍스트가 길수록 중요한 단어와 덜 중요한 단어가 섞여있다.\n",
    "   - 문맥 벡터만으로는 오래된 단어의 정보를 기억히기 힘들다.\n",
    "   - 그래디언트가 여러 다임스텝을 걸져 전파되면서 점점 약해지기 때문이다.\n",
    "      - 타임스텝: RNN이 하나의 단어를 시간의 순서에 따라 처리하는 과정\n",
    "\n",
    "8. 어텐션 메커니즘\n",
    "   - 인코더의 마지막 타임스텝에서 얻은 은닉 상태 참조\n",
    "   - 인코더의 모든 은닉 상태를  디코더가 텍스트를 생성할 때마다 참조. \n",
    "   - 무장에서 특히 중요한 단어들에 더 집중하도록 도와줌으로써 번역의 정확도를 높인다.\n",
    "   - 디코더는 번역 문장을 만들 때 어떤 단어에 주의(어텐션)을 기울여야할지 결정한다.\n",
    "\n",
    "      ![어텐션](image/04-01-atteition.png)   \n",
    "\n",
    "   - 디코더가 '모든' 타임스텝에서 이코더의 은닉 상태를 참고해 문맥 벡터를 만든다.\n",
    "   - 수식에서 볼 수 있듯이 문맥 벡터는 인코더가 각 타임스템에서 생성하는 모든 은닉 상태의*가중치의 합이다.\n",
    "      - 디코더는 가중치 a<sub>ij</sub>를 통해 어떤 은닉 상택, 즉 어떤 단어에 주의를 기울일지 정할 수 있다.\n",
    "\n",
    "      ![어텐션 수식](image/04-01-atteition_math.png)   \n",
    "\n",
    "   - 인코더의 은닉 상태에 곱해지는 가중치는 아래와 같은 수식을 통해 훈련된다.\n",
    "\n",
    "      ![수식 학습](image/04-01-atteition_math2.png)\n",
    "\n",
    "   - 최종적으로 계산된 e<sub>ij</sub>의 결과를 다음과 같은 소프트맥스 함수에 통과시켜 가중치 a<sub>ij</sub>를 얻는다.\n",
    "      - 소프트맥스 함수를 사용했기 때문에 모든합은 1이 된다.\n",
    "\n",
    "      ![가중치 수식](image/04-01-atteition_math3.png)\n",
    "\n",
    "9. 이렇게 인코더의 은닉 상태와 디코더의 은닉 상태를 더하는 어텐션은 논문 발쵸자의 이름을 따서 바흐다나우 어덴션이라고 부르기도한다.\n",
    "   - 인코더의 출려과 디코더의 출력을 더하기 때문에 덧셈 어텐션, 또는 연결 어텐션이라고 부르기도한다.\n",
    "\n",
    "10. 루옹 어텐션(Luong attention) / 점곱 어텐션\n",
    "   - 인코더의 최종 은닉 상태와 디코더의 은닉 상태를 곱하는 방식으로 a<sub>ij</sub>를 계산한다.\n",
    "\n",
    "      ![점곱 어텐션](image/04-01-atteition_math4.png)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
